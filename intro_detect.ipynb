{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HZEoKUe9IhQA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import json\n",
        "import os\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip train_data.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2olAttiOItu5",
        "outputId": "62721ee6-a027-415a-f691-388c09faa40f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  train_data.zip\n",
            "   creating: train_data/\n",
            "   creating: train_data/-220020068_456255399/\n",
            "  inflating: train_data/-220020068_456255399/video_tensor.pt  \n",
            "  inflating: train_data/-220020068_456255399/audio_tensor.pt  \n",
            "  inflating: train_data/labels.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = '/content/train_data'"
      ],
      "metadata": {
        "id": "ftCEzWNPJBU9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class IntroDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data_path):\n",
        "        super().__init__()\n",
        "\n",
        "        self.data_path = data_path\n",
        "        _, self.video_names, self.labels_name = next(os.walk(data_path))\n",
        "\n",
        "        with open(f'{train_path}/{self.labels_name[0]}') as f:\n",
        "            self.labels = json.load(f)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_names)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        video_frames = torch.load(f\"{self.data_path}/{self.video_names[index]}/video_tensor.pt\").to(torch.float32)\n",
        "        audio_frames = torch.load(f\"{self.data_path}/{self.video_names[index]}/audio_tensor.pt\").to(torch.float32)\n",
        "\n",
        "        label = torch.tensor(self.labels[self.video_names[index]], dtype=int)\n",
        "\n",
        "        return {'video': video_frames,\n",
        "                'audio': audio_frames,\n",
        "                'label': label}\n"
      ],
      "metadata": {
        "id": "sAWjX8zIJoX5"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "intro_dataset = IntroDataset(train_path)"
      ],
      "metadata": {
        "id": "Cn_vu3hWL9P3"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class AudioBackbone(nn.Module):\n",
        "    def __init__(self, filters_num):\n",
        "        super().__init__()\n",
        "\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv1d(filters_num, 32, kernel_size=5, padding='same'),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.MaxPool1d(4)\n",
        "        )\n",
        "\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv1d(32, 64, kernel_size=5, padding='same'),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.MaxUnpool1d(5)\n",
        "        )\n",
        "\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Conv1d(64, 128, kernel_size=5, padding='same'),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.MaxUnpool1d(5)\n",
        "        )\n",
        "\n",
        "    def forward(self, audio):\n",
        "        audio = self.block1(audio)\n",
        "        audio = self.block2(audio)\n",
        "        audio = self.block3(audio)\n",
        "\n",
        "        return audio"
      ],
      "metadata": {
        "id": "aKS-kUHZNxhV"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "\n",
        "class VideoBackbone(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.resnet = torchvision.models.resnet50()\n",
        "        self.resnet.avgpool = nn.Identity()\n",
        "        self.resnet.fc = nn.Identity()\n",
        "\n",
        "    def forward(self, video):\n",
        "        return self.resnet(video)"
      ],
      "metadata": {
        "id": "a1Q4kBwaaSx3"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "intro_dataset[0]['audio'].size(), intro_dataset[0]['video'].size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZQewT8hbp3u",
        "outputId": "6d8771ee-c924-42c4-d4c9-f1a6ebab3696"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([240, 128, 1600]), torch.Size([240, 3, 224, 224]))"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.zeros((3, 4, 5, 6))\n",
        "a.reshape((12, -1)).size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaznVi2jcGJ9",
        "outputId": "49d2ddc2-0c7d-4f55-e299-a873baf83bd3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12, 30])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class IntroDetecter(nn.Module):\n",
        "\n",
        "    def __init__(self, mel_filters_num):\n",
        "        super().__init__()\n",
        "        self.mel_filters_num = mel_filters_num\n",
        "\n",
        "\n",
        "        self.audio_backbone = AudioBackbone(mel_filters_num)\n",
        "        self.video_backbone = VideoBackbone()\n",
        "\n",
        "        #self.\n",
        "\n",
        "    def forward(self, video, audio):\n",
        "        batch_size, T, img_channels, h, w = video.size()\n",
        "        time_points_num = audio.size(2)\n",
        "\n",
        "        audio = audio.reshape((batch_size * T, self.mel_filters_num, time_points_num))\n",
        "        video = video.reshape((batch_size * T, img_channels, h, w))\n",
        "\n",
        "        audio_features = self.audio_backbone(audio)\n",
        "        video_features = self.video_backbone(video)\n",
        "\n",
        "        audio = audio.reshape((batch_size, T, 128))\n",
        "        video = video.reshape((batch_size, T, 512))\n",
        "\n",
        "        features = torch.cat((audio, video), dim=2)\n",
        "\n",
        "        # дальше должен быть b-lstm-crf на этих фичах\n",
        "        # https://docs.pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html"
      ],
      "metadata": {
        "id": "ETf0i2rQbAgc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}